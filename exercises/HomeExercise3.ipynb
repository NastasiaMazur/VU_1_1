{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NastasiaMazur/VU_1_1/blob/main/exercises/HomeExercise3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Home Exercise 3: Creating Neural Model in PyTorch\n",
        "\n",
        "In this home exercise, you will first learn how to create a neural model in PyTorch and then you will train and improve a mini-implementation of an embedding model.\n"
      ],
      "metadata": {
        "id": "uoESZ6QpJqlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Exercise 3a: Neural Network Model**\n",
        "\n",
        "First we need to import the neural network module of PyTorch:"
      ],
      "metadata": {
        "id": "816q6jCdJ9sP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "cPYVg894JmTc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use `nn.Linear(H_in, H_out)` to create a a linear layer. This will take a matrix of `(N, *, H_in)` dimensions and output a matrix of `(N, *, H_out)`. The `*` denotes that there could be arbitrary number of dimensions in between. The linear layer performs the operation `Ax+b`, where `A` and `b` are initialized randomly. If we don't want the linear layer to learn the bias parameters, we can initialize our layer with `bias=False`."
      ],
      "metadata": {
        "id": "NvK5KMXlad5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the inputs\n",
        "input = torch.ones(2,3,4)\n",
        "print(\"Input \", input)\n",
        "\n",
        "# N* H_in -> N*H_out\n",
        "linear = nn.Linear(4, 2)\n",
        "linear_output = linear(input)\n",
        "linear_output"
      ],
      "metadata": {
        "id": "TSt4jv5Dag8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "293d8faa-0eaf-493d-f1e6-c490503eeccc"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input  tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.0148,  0.5369],\n",
              "         [-1.0148,  0.5369],\n",
              "         [-1.0148,  0.5369]],\n",
              "\n",
              "        [[-1.0148,  0.5369],\n",
              "         [-1.0148,  0.5369],\n",
              "         [-1.0148,  0.5369]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(linear.parameters()) # Ax + b"
      ],
      "metadata": {
        "id": "nxUN1Nf0a_Ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2e1541b-3b2a-4ad6-8cd5-9a755aa6f0f5"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-4.9833e-01,  2.5949e-01, -4.8202e-01,  1.8706e-01],\n",
              "         [ 4.8114e-01, -3.6670e-01,  5.3942e-02, -1.3202e-04]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.4810,  0.3686], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add an activation function:"
      ],
      "metadata": {
        "id": "hPBSQbAQbJ-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid = nn.Sigmoid()\n",
        "output = sigmoid(linear_output)\n",
        "output"
      ],
      "metadata": {
        "id": "GbEo76ejbLb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e43dd818-e3f2-444b-a338-d84e6fb83a59"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.2660, 0.6311],\n",
              "         [0.2660, 0.6311],\n",
              "         [0.2660, 0.6311]],\n",
              "\n",
              "        [[0.2660, 0.6311],\n",
              "         [0.2660, 0.6311],\n",
              "         [0.2660, 0.6311]]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of creating intermediate layers and passing variables around, we can create a sequence:"
      ],
      "metadata": {
        "id": "sawC0L7lbcZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block = nn.Sequential(\n",
        "    nn.Linear(4, 2),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "input = torch.ones(2,3,4)\n",
        "output = block(input)\n",
        "output"
      ],
      "metadata": {
        "id": "XinqnT6TbhtY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d05d9f-3353-43c8-e715-486911643a97"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4650, 0.6389],\n",
              "         [0.4650, 0.6389],\n",
              "         [0.4650, 0.6389]],\n",
              "\n",
              "        [[0.4650, 0.6389],\n",
              "         [0.4650, 0.6389],\n",
              "         [0.4650, 0.6389]]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Exercise 3b: Word Embeddings**\n"
      ],
      "metadata": {
        "id": "_D_jInC4dMOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of using predefined modules of nn we can define our own modules and build custom neural networks. As a toy example, we will convert words to word embeddings. The preprocessing below should be done more elegantly and not as simply as below."
      ],
      "metadata": {
        "id": "uEWtiwRybkj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# We will use Shakespeare Sonnet 2\n",
        "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\"\n",
        "\n",
        "training_sentence = test_sentence.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
        "print(training_sentence)"
      ],
      "metadata": {
        "id": "HwIWa0p4dkH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56e20a23-42e1-49d3-902f-c1895112be9f"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['when', 'forty', 'winters', 'shall', 'besiege', 'thy', 'brow', 'and', 'dig', 'deep', 'trenches', 'in', 'thy', 'beautys', 'field', 'thy', 'youths', 'proud', 'livery', 'so', 'gazed', 'on', 'now', 'will', 'be', 'a', 'totterd', 'weed', 'of', 'small', 'worth', 'held', 'then', 'being', 'asked', 'where', 'all', 'thy', 'beauty', 'lies', 'where', 'all', 'the', 'treasure', 'of', 'thy', 'lusty', 'days', 'to', 'say', 'within', 'thine', 'own', 'deep', 'sunken', 'eyes', 'were', 'an', 'alleating', 'shame', 'and', 'thriftless', 'praise', 'how', 'much', 'more', 'praise', 'deservd', 'thy', 'beautys', 'use', 'if', 'thou', 'couldst', 'answer', 'this', 'fair', 'child', 'of', 'mine', 'shall', 'sum', 'my', 'count', 'and', 'make', 'my', 'old', 'excuse', 'proving', 'his', 'beauty', 'by', 'succession', 'thine', 'this', 'were', 'to', 'be', 'new', 'made', 'when', 'thou', 'art', 'old', 'and', 'see', 'thy', 'blood', 'warm', 'when', 'thou', 'feelst', 'it', 'cold']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next let's find our vocabulary, i.e., all the unique words in the training data:"
      ],
      "metadata": {
        "id": "ZxO3KXxUflVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = set(w for w in training_sentence)\n",
        "vocabulary"
      ],
      "metadata": {
        "id": "cINJIi0NgLIr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7420b83-126b-4288-9fb5-788ddeb06e83"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'all',\n",
              " 'alleating',\n",
              " 'an',\n",
              " 'and',\n",
              " 'answer',\n",
              " 'art',\n",
              " 'asked',\n",
              " 'be',\n",
              " 'beauty',\n",
              " 'beautys',\n",
              " 'being',\n",
              " 'besiege',\n",
              " 'blood',\n",
              " 'brow',\n",
              " 'by',\n",
              " 'child',\n",
              " 'cold',\n",
              " 'couldst',\n",
              " 'count',\n",
              " 'days',\n",
              " 'deep',\n",
              " 'deservd',\n",
              " 'dig',\n",
              " 'excuse',\n",
              " 'eyes',\n",
              " 'fair',\n",
              " 'feelst',\n",
              " 'field',\n",
              " 'forty',\n",
              " 'gazed',\n",
              " 'held',\n",
              " 'his',\n",
              " 'how',\n",
              " 'if',\n",
              " 'in',\n",
              " 'it',\n",
              " 'lies',\n",
              " 'livery',\n",
              " 'lusty',\n",
              " 'made',\n",
              " 'make',\n",
              " 'mine',\n",
              " 'more',\n",
              " 'much',\n",
              " 'my',\n",
              " 'new',\n",
              " 'now',\n",
              " 'of',\n",
              " 'old',\n",
              " 'on',\n",
              " 'own',\n",
              " 'praise',\n",
              " 'proud',\n",
              " 'proving',\n",
              " 'say',\n",
              " 'see',\n",
              " 'shall',\n",
              " 'shame',\n",
              " 'small',\n",
              " 'so',\n",
              " 'succession',\n",
              " 'sum',\n",
              " 'sunken',\n",
              " 'the',\n",
              " 'then',\n",
              " 'thine',\n",
              " 'this',\n",
              " 'thou',\n",
              " 'thriftless',\n",
              " 'thy',\n",
              " 'to',\n",
              " 'totterd',\n",
              " 'treasure',\n",
              " 'trenches',\n",
              " 'use',\n",
              " 'warm',\n",
              " 'weed',\n",
              " 'were',\n",
              " 'when',\n",
              " 'where',\n",
              " 'will',\n",
              " 'winters',\n",
              " 'within',\n",
              " 'worth',\n",
              " 'youths'}"
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We introduce a special token, `<unk>`, to tackle the words that are out of vocabulary. We could pick another string for our unknown token if we wanted. The only requirement here is that our token should be unique: we should only be using this token for unknown words. We will also add this special token to our vocabulary."
      ],
      "metadata": {
        "id": "PyJFHmLHgdMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary.add(\"<unk>\")"
      ],
      "metadata": {
        "id": "B52a3cgage0n"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will create the index for our vocabulary - one index to word and one word to index to make looking up words easier:"
      ],
      "metadata": {
        "id": "0rxs8FuZha0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ix_to_word = sorted(list(vocabulary))\n",
        "word_to_ix = {word: ind for ind, word in enumerate(ix_to_word)}\n",
        "word_to_ix"
      ],
      "metadata": {
        "id": "ACclVi0PhhC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51d13d6b-ba42-403c-d771-53676e9af836"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<unk>': 0,\n",
              " 'a': 1,\n",
              " 'all': 2,\n",
              " 'alleating': 3,\n",
              " 'an': 4,\n",
              " 'and': 5,\n",
              " 'answer': 6,\n",
              " 'art': 7,\n",
              " 'asked': 8,\n",
              " 'be': 9,\n",
              " 'beauty': 10,\n",
              " 'beautys': 11,\n",
              " 'being': 12,\n",
              " 'besiege': 13,\n",
              " 'blood': 14,\n",
              " 'brow': 15,\n",
              " 'by': 16,\n",
              " 'child': 17,\n",
              " 'cold': 18,\n",
              " 'couldst': 19,\n",
              " 'count': 20,\n",
              " 'days': 21,\n",
              " 'deep': 22,\n",
              " 'deservd': 23,\n",
              " 'dig': 24,\n",
              " 'excuse': 25,\n",
              " 'eyes': 26,\n",
              " 'fair': 27,\n",
              " 'feelst': 28,\n",
              " 'field': 29,\n",
              " 'forty': 30,\n",
              " 'gazed': 31,\n",
              " 'held': 32,\n",
              " 'his': 33,\n",
              " 'how': 34,\n",
              " 'if': 35,\n",
              " 'in': 36,\n",
              " 'it': 37,\n",
              " 'lies': 38,\n",
              " 'livery': 39,\n",
              " 'lusty': 40,\n",
              " 'made': 41,\n",
              " 'make': 42,\n",
              " 'mine': 43,\n",
              " 'more': 44,\n",
              " 'much': 45,\n",
              " 'my': 46,\n",
              " 'new': 47,\n",
              " 'now': 48,\n",
              " 'of': 49,\n",
              " 'old': 50,\n",
              " 'on': 51,\n",
              " 'own': 52,\n",
              " 'praise': 53,\n",
              " 'proud': 54,\n",
              " 'proving': 55,\n",
              " 'say': 56,\n",
              " 'see': 57,\n",
              " 'shall': 58,\n",
              " 'shame': 59,\n",
              " 'small': 60,\n",
              " 'so': 61,\n",
              " 'succession': 62,\n",
              " 'sum': 63,\n",
              " 'sunken': 64,\n",
              " 'the': 65,\n",
              " 'then': 66,\n",
              " 'thine': 67,\n",
              " 'this': 68,\n",
              " 'thou': 69,\n",
              " 'thriftless': 70,\n",
              " 'thy': 71,\n",
              " 'to': 72,\n",
              " 'totterd': 73,\n",
              " 'treasure': 74,\n",
              " 'trenches': 75,\n",
              " 'use': 76,\n",
              " 'warm': 77,\n",
              " 'weed': 78,\n",
              " 'were': 79,\n",
              " 'when': 80,\n",
              " 'where': 81,\n",
              " 'will': 82,\n",
              " 'winters': 83,\n",
              " 'within': 84,\n",
              " 'worth': 85,\n",
              " 'youths': 86}"
            ]
          },
          "metadata": {},
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ How can we now lookup which word is the fifth word in our index list?"
      ],
      "metadata": {
        "id": "dIEa5IZ8hn7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "\n",
        "fifth_word = ix_to_word[4]\n",
        "fifth_word\n",
        "\n",
        "# I considered the 0 index as an OOV word\n"
      ],
      "metadata": {
        "id": "ObGeIzeTh3MC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "53db7119-3da6-443f-f674-8ab8f6035054"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'an'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a very simple solution of building trigrams to train our model."
      ],
      "metadata": {
        "id": "-4WXk4kpnRyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigrams = [([training_sentence[i], training_sentence[i + 1]], training_sentence[i + 2])\n",
        "            for i in range(len(training_sentence)-2)]\n",
        "print(trigrams)"
      ],
      "metadata": {
        "id": "bXpIVBjpiyl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df758066-77f7-408e-daeb-e3488dc8dbd2"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['when', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege'), (['shall', 'besiege'], 'thy'), (['besiege', 'thy'], 'brow'), (['thy', 'brow'], 'and'), (['brow', 'and'], 'dig'), (['and', 'dig'], 'deep'), (['dig', 'deep'], 'trenches'), (['deep', 'trenches'], 'in'), (['trenches', 'in'], 'thy'), (['in', 'thy'], 'beautys'), (['thy', 'beautys'], 'field'), (['beautys', 'field'], 'thy'), (['field', 'thy'], 'youths'), (['thy', 'youths'], 'proud'), (['youths', 'proud'], 'livery'), (['proud', 'livery'], 'so'), (['livery', 'so'], 'gazed'), (['so', 'gazed'], 'on'), (['gazed', 'on'], 'now'), (['on', 'now'], 'will'), (['now', 'will'], 'be'), (['will', 'be'], 'a'), (['be', 'a'], 'totterd'), (['a', 'totterd'], 'weed'), (['totterd', 'weed'], 'of'), (['weed', 'of'], 'small'), (['of', 'small'], 'worth'), (['small', 'worth'], 'held'), (['worth', 'held'], 'then'), (['held', 'then'], 'being'), (['then', 'being'], 'asked'), (['being', 'asked'], 'where'), (['asked', 'where'], 'all'), (['where', 'all'], 'thy'), (['all', 'thy'], 'beauty'), (['thy', 'beauty'], 'lies'), (['beauty', 'lies'], 'where'), (['lies', 'where'], 'all'), (['where', 'all'], 'the'), (['all', 'the'], 'treasure'), (['the', 'treasure'], 'of'), (['treasure', 'of'], 'thy'), (['of', 'thy'], 'lusty'), (['thy', 'lusty'], 'days'), (['lusty', 'days'], 'to'), (['days', 'to'], 'say'), (['to', 'say'], 'within'), (['say', 'within'], 'thine'), (['within', 'thine'], 'own'), (['thine', 'own'], 'deep'), (['own', 'deep'], 'sunken'), (['deep', 'sunken'], 'eyes'), (['sunken', 'eyes'], 'were'), (['eyes', 'were'], 'an'), (['were', 'an'], 'alleating'), (['an', 'alleating'], 'shame'), (['alleating', 'shame'], 'and'), (['shame', 'and'], 'thriftless'), (['and', 'thriftless'], 'praise'), (['thriftless', 'praise'], 'how'), (['praise', 'how'], 'much'), (['how', 'much'], 'more'), (['much', 'more'], 'praise'), (['more', 'praise'], 'deservd'), (['praise', 'deservd'], 'thy'), (['deservd', 'thy'], 'beautys'), (['thy', 'beautys'], 'use'), (['beautys', 'use'], 'if'), (['use', 'if'], 'thou'), (['if', 'thou'], 'couldst'), (['thou', 'couldst'], 'answer'), (['couldst', 'answer'], 'this'), (['answer', 'this'], 'fair'), (['this', 'fair'], 'child'), (['fair', 'child'], 'of'), (['child', 'of'], 'mine'), (['of', 'mine'], 'shall'), (['mine', 'shall'], 'sum'), (['shall', 'sum'], 'my'), (['sum', 'my'], 'count'), (['my', 'count'], 'and'), (['count', 'and'], 'make'), (['and', 'make'], 'my'), (['make', 'my'], 'old'), (['my', 'old'], 'excuse'), (['old', 'excuse'], 'proving'), (['excuse', 'proving'], 'his'), (['proving', 'his'], 'beauty'), (['his', 'beauty'], 'by'), (['beauty', 'by'], 'succession'), (['by', 'succession'], 'thine'), (['succession', 'thine'], 'this'), (['thine', 'this'], 'were'), (['this', 'were'], 'to'), (['were', 'to'], 'be'), (['to', 'be'], 'new'), (['be', 'new'], 'made'), (['new', 'made'], 'when'), (['made', 'when'], 'thou'), (['when', 'thou'], 'art'), (['thou', 'art'], 'old'), (['art', 'old'], 'and'), (['old', 'and'], 'see'), (['and', 'see'], 'thy'), (['see', 'thy'], 'blood'), (['thy', 'blood'], 'warm'), (['blood', 'warm'], 'when'), (['warm', 'when'], 'thou'), (['when', 'thou'], 'feelst'), (['thou', 'feelst'], 'it'), (['feelst', 'it'], 'cold')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NGramLanguageModeler(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, window_size, hidden_dim):\n",
        "        super(NGramLanguageModeler, self).__init__()          # calls the constructor of the parent class (nn.Module) to initialize the model\n",
        "\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim) # creates an embedding layer;  converts input indices into dense vectors\n",
        "        self.hidden_layer = nn.Sequential(                        # hidden layer of the model as a sequential container\n",
        "            nn.Linear(window_size * embedding_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.output_layer = nn.Linear(hidden_dim, vocab_size)     # creates the output layer; takes the hidden representation and produces output scores for each word in the vocabulary\n",
        "\n",
        "    def forward(self, inputs):                            # specifies how the input data should be processed through the layers to produce the output\n",
        "        embeds = self.embeddings(inputs).view((1, -1))    # takes the input indices inputs, passes them through the embedding layer, and flattens the resulting tensor into a 1D tensor\n",
        "        layer1 = self.hidden_layer(embeds)                # passes the flattened embeddings through the hidden layer, applying the linear transformation followed by the ReLU activation\n",
        "        output = self.output_layer(layer1)                # passes the result of the hidden layer through the output layer, producing the final output scores for each word in the vocabulary\n",
        "        probabilities = nn.functional.log_softmax(output, dim=1)\n",
        "        return probabilities"
      ],
      "metadata": {
        "id": "_YJ-w7APcUO0"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ Now let's train the model. Try to adapt the hyperparamters so that the total_loss is reduced. These include in the initial settings:\n",
        "\n",
        "*   Dimensionality of the embeddings: 10\n",
        "*   Dimensionality of the hidden layer: 128\n",
        "*   Number of epochs: 10\n",
        "*   Learning rate: 0.002\n",
        "*   Loss function (`NLLLoss()` Negative Log Likelihood right now)\n",
        "\n",
        "The window size can in this toy example not be changed, since we train on trigrams (so we only have two context words in the set).\n",
        "\n",
        "The code cell before, the `class NGramLanguageModeler(nn.Module)` defines the exact setup of the network. For the setup one thing to change could be the  activation functions (ReLU and Log-Softmax right now) - others can be found [here](https://pytorch.org/docs/stable/nn.html) for sequential layers and here for other uses as [nn.functional](https://pytorch.org/docs/stable/nn.functional.html)).\n",
        "\n",
        "Just for comparison, this is the initial output with the first settings of the notebook:\n",
        "\n",
        "`See how loss decreases with each epoch:  [4.502952638980561, 4.453238930322428, 4.404419487556525, 4.356399970771992, 4.309086953644204, 4.262444785210938, 4.216389432417608, 4.170906921403598, 4.125928629816106, 4.081349758975274]\n",
        "Loss of the last epoch: 4.081349758975274`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yCuqOvyfxS3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 2\n",
        "#embedding_dim = 10\n",
        "embedding_dim = 90\n",
        "#hidden_dim = 128\n",
        "hidden_dim = 310\n",
        "num_epochs = 30\n",
        "\n",
        "losses = []\n",
        "# Negative log likelihood\n",
        "loss_function = nn.NLLLoss()\n",
        "ngram_model = NGramLanguageModeler(len(vocabulary), embedding_dim, window_size, hidden_dim)\n",
        "\n",
        "# What do SGD and lr mean? What happenes if you change them?\n",
        "optimizer = torch.optim.SGD(ngram_model.parameters(), lr=0.002)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for context, target in trigrams:\n",
        "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
        "        # into integer indices and wrap them in tensors)\n",
        "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
        "\n",
        "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
        "        # new instance, you need to zero out the gradients from the old\n",
        "        # instance\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Step 3. Run the forward pass, getting probabilities over next\n",
        "        # words - the size of this tensor is 87 corresponding to the size of the vocabulary\n",
        "        probabilities = ngram_model.forward(context_idxs)\n",
        "\n",
        "        # Step 4. Compute your loss function. The target word (gold standard label) needs to be\n",
        "        # wrapped in a tensor.\n",
        "        loss = loss_function(probabilities, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "\n",
        "        # Step 5. Do the backward pass and update the gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "        total_loss += loss.item()\n",
        "    losses.append(total_loss / len(trigrams))\n",
        "\n",
        "print(\"See how loss decreases with each epoch: \", losses)\n",
        "print(\"Loss of the last epoch:\", losses[-1])"
      ],
      "metadata": {
        "id": "Q9jWtyRkjI7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd2d12c7-18d7-4162-df8f-3af9dff53740"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "See how loss decreases with each epoch:  [4.498052320649139, 4.353869965646119, 4.21265608863493, 4.073893749608403, 3.9368296281426356, 3.800809022599617, 3.6654945263820413, 3.5305067332444993, 3.3954024135538963, 3.2600987379529833, 3.124404770083132, 2.9883737205404097, 2.8520351732726645, 2.71572950930722, 2.579567740976283, 2.4437718249000278, 2.309051687211062, 2.175646495502607, 2.044284159103326, 1.915521355593099, 1.7897181637519228, 1.6675243591312814, 1.5493410464409179, 1.4358343352786207, 1.3273726295580905, 1.2243544963081325, 1.1271895125376439, 1.036036624977019, 0.9512852571443119, 0.8728797986971594]\n",
            "Loss of the last epoch: 0.8728797986971594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ Implement the cosine simmilarity for the trained embeddings. For this you need to choose too find the index of the two words you wish to compare, get their embeddings, convert them into numpy arrays, and run them through the cosine function provided."
      ],
      "metadata": {
        "id": "-vNFAngV4S3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "word1 = \"beauty\"\n",
        "word2 = \"treasure\"\n",
        "index_word1 = word_to_ix[word1]\n",
        "index_word2 = word_to_ix[word2]\n",
        "\n",
        "# Tensor to look up specific embeddings\n",
        "lookup_tensor = torch.tensor(list(word_to_ix.values()), dtype=torch.long)\n",
        "\n",
        "# Embedding for the 10th word1 (beauty) word in the vocabulary:\n",
        "lookup_embeds_word1 = ngram_model.embeddings(lookup_tensor)[index_word1]         #[10]\n",
        "print(f\"Embedding for word '{word1}':\\n {lookup_embeds_word1}\\n\")\n",
        "\n",
        "# Embedding for the 74th (treasure) word in the vocabulary:\n",
        "lookup_embeds_word2 = ngram_model.embeddings(lookup_tensor)[index_word2]         #[74]\n",
        "print(f\"Embedding for word '{word2}':\\n {lookup_embeds_word2}\\n\")\n",
        "\n",
        "# Convert the embedding tensor to a NumPy array:\n",
        "embedding_array_word1 = lookup_embeds_word1.detach().numpy()\n",
        "print(f\"Embedding for word '{word1}' as NumPy array:\\n {embedding_array_word1}\\n\")\n",
        "\n",
        "embedding_array_word2 = lookup_embeds_word2.detach().numpy()\n",
        "print(f\"Embedding for word '{word2}' as NumPy array:\\n {embedding_array_word2}\\n\")\n",
        "\n",
        "print(\"__________________________________________________\")\n",
        "\n",
        "#cosine = np.dot(A,B)/(norm(A, axis=1)*norm(B))\n",
        "cosine = np.dot(embedding_array_word1, embedding_array_word2) / (norm(embedding_array_word1, axis=0) * norm(embedding_array_word2))\n",
        "\n",
        "print(f\"Cosine similarity between '{word1}' and '{word2}':\\n{cosine}\")\n",
        "print(cosine) #Why there is a difference? --- might not display all the decimal places\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGLcgKIH4rn1",
        "outputId": "8f668530-e1dd-4805-be6b-4606354a40b1"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for word 'beauty':\n",
            " tensor([ 0.1924,  1.1404,  0.9205,  0.4104, -1.4681, -0.0266, -0.1656,  1.4706,\n",
            "         0.5671,  0.7361,  1.5322,  1.3071,  0.0046,  1.9889, -0.3399, -1.3688,\n",
            "         2.1025, -1.3657, -1.2693, -1.4600,  0.3203, -1.8196, -0.3971, -0.5406,\n",
            "         0.5845,  0.9939, -0.2960, -0.2596,  0.7936,  0.0383, -0.5280, -0.4449,\n",
            "         2.1562,  0.9122, -2.2351,  2.5978, -3.2343,  1.7922,  0.3024,  0.2582,\n",
            "        -1.0259,  1.4170,  0.0804, -0.3188, -1.2834,  0.3329,  1.6604,  0.7046,\n",
            "        -2.2751, -0.4170,  0.0327, -1.4098, -0.0951, -0.2115,  1.1788,  2.0130,\n",
            "         0.5873, -0.0055,  1.0064,  0.3590, -0.1866, -0.0072, -0.0849,  0.2422,\n",
            "        -1.0868, -0.3723, -1.3010,  0.0729, -1.0451,  1.3904,  0.0859, -0.9546,\n",
            "         1.1792,  2.2133, -0.9367,  1.3256,  1.0568, -0.4365,  0.7645, -1.5093,\n",
            "        -0.1162,  0.5272, -0.1372,  0.0839,  0.0098, -1.9132,  0.3655, -0.3679,\n",
            "         0.9619, -0.2504], grad_fn=<SelectBackward0>)\n",
            "\n",
            "Embedding for word 'treasure':\n",
            " tensor([-0.6016, -0.2083,  1.7596,  0.3536,  0.8991, -0.9298, -0.0819,  0.2768,\n",
            "        -0.0559, -0.4120,  0.9483, -1.0723, -1.6818, -0.3291, -1.4385,  0.5584,\n",
            "        -0.3608,  1.1775,  0.4310,  0.8647,  0.7476,  1.3407,  0.0621, -0.0339,\n",
            "         0.9969,  1.0452,  2.4410,  0.0934,  0.2005, -0.0124, -0.8461,  0.3489,\n",
            "        -0.4892,  0.1104,  0.4899, -0.1858, -0.6923, -0.0509,  1.2125, -0.3110,\n",
            "        -0.4143, -1.3480, -2.1049,  0.0593, -0.4469,  0.8304, -0.5440,  0.9908,\n",
            "         2.9274, -0.1989,  0.0765,  0.6507,  1.2799,  0.3207,  0.4276,  0.9142,\n",
            "         0.0693, -0.0879, -0.6564, -2.2398,  0.4797,  0.2796, -1.6077,  0.4398,\n",
            "         0.9389, -0.1168,  0.6893,  0.1682,  2.2908,  0.2471,  0.3441,  0.3456,\n",
            "         0.8917, -0.0083,  1.1020,  1.9767, -1.2812, -1.0924, -1.3530, -2.6609,\n",
            "         1.0403, -0.2295, -1.4129,  1.3445,  0.3491,  2.1065,  1.6397,  2.2172,\n",
            "        -1.7069, -0.4829], grad_fn=<SelectBackward0>)\n",
            "\n",
            "Embedding for word 'beauty' as NumPy array:\n",
            " [ 0.19235437  1.1404054   0.92052543  0.41035032 -1.468053   -0.02663271\n",
            " -0.16564277  1.470606    0.56709224  0.7360971   1.532152    1.307142\n",
            "  0.00459964  1.9888825  -0.33991283 -1.3687577   2.1025417  -1.3657093\n",
            " -1.2693216  -1.459959    0.32030857 -1.8196313  -0.39713544 -0.5405912\n",
            "  0.5844642   0.9939364  -0.29595998 -0.25961933  0.79364455  0.03834147\n",
            " -0.5280183  -0.4448721   2.1561794   0.9121698  -2.2351115   2.5977829\n",
            " -3.2343493   1.7921543   0.30238816  0.25816184 -1.0258615   1.416984\n",
            "  0.08036602 -0.31881183 -1.2834102   0.33288848  1.6603909   0.7046379\n",
            " -2.275101   -0.4169989   0.03268487 -1.409826   -0.0950981  -0.21149004\n",
            "  1.1787546   2.0130024   0.58734304 -0.00552076  1.0063537   0.35896733\n",
            " -0.18660302 -0.0072044  -0.08491134  0.24218625 -1.0867555  -0.37232056\n",
            " -1.3010451   0.07292793 -1.0451058   1.3904357   0.085873   -0.95461065\n",
            "  1.1791668   2.2132978  -0.9367287   1.325643    1.05683    -0.4365146\n",
            "  0.76447475 -1.5092566  -0.11615915  0.5271832  -0.1371806   0.08393984\n",
            "  0.00982521 -1.9131573   0.36548167 -0.36786267  0.96193033 -0.2504093 ]\n",
            "\n",
            "Embedding for word 'treasure' as NumPy array:\n",
            " [-0.6015925  -0.20832936  1.7596229   0.3535679   0.8990882  -0.9297573\n",
            " -0.08185263  0.27676007 -0.05587314 -0.41204953  0.9482979  -1.0722944\n",
            " -1.6818146  -0.32914943 -1.4384571   0.5583675  -0.36077854  1.177499\n",
            "  0.43096468  0.8646513   0.7475635   1.3406938   0.06213383 -0.03385222\n",
            "  0.9969135   1.0451523   2.4409893   0.09341639  0.2004671  -0.01238323\n",
            " -0.8461259   0.34890863 -0.48923957  0.11039607  0.48986948 -0.18576753\n",
            " -0.69228643 -0.05087507  1.2124897  -0.31103587 -0.41429043 -1.3479878\n",
            " -2.1048994   0.0593386  -0.44688547  0.8303621  -0.5440437   0.9908409\n",
            "  2.927402   -0.19891542  0.07652684  0.6507133   1.279867    0.32069966\n",
            "  0.4276088   0.91424733  0.06931499 -0.08787698 -0.6564214  -2.23977\n",
            "  0.4797471   0.2796015  -1.6076527   0.43975893  0.9389045  -0.11684157\n",
            "  0.6892504   0.16822167  2.29079     0.24709886  0.34405163  0.3455701\n",
            "  0.89174145 -0.00831127  1.1020054   1.9767104  -1.281164   -1.0924323\n",
            " -1.3530433  -2.6609173   1.0402652  -0.22954999 -1.4129052   1.3445239\n",
            "  0.3490605   2.1064887   1.6397141   2.2171965  -1.7069167  -0.48294768]\n",
            "\n",
            "__________________________________________________\n",
            "Cosine similarity between 'beauty' and 'treasure':\n",
            "-0.17296256124973297\n",
            "-0.17296256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RkvXVVFKkkA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comment:**\n",
        "\n",
        "For the purposes of visualization, I have decided to use words instead of just indexes in the code. Although I think it would make sense to do the opposite for a more complex program.\n",
        "\n",
        "Also by using axis=1 it appeared the error message:\n",
        "\n",
        "AxisError: axis 1 is out of bounds for array of dimension 1"
      ],
      "metadata": {
        "id": "fT75WdWwhCMo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3i-3tBsihFLC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}