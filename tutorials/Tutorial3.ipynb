{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NastasiaMazur/VU_1_1/blob/main/tutorials/Tutorial3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GszGq3W2gEWm"
      },
      "source": [
        "# Tutorial 3: Introduction to Computational Linguistics\n",
        "\n",
        "This is the second tutorial with practical exercises for the lecture Introduction to Computational Linguistics in the winter semester 2023. Hands-on exercises are marked with ðŸ‘‹ âš’ and questions are marked with â“. Remember to first **store this notebook** in your Drive or GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnC6HAtqgHGK"
      },
      "source": [
        "---\n",
        "\n",
        "## **Lesson 3: Word Embeddings**\n",
        "\n",
        "A vector representation of words trained with a neural network is called a word embedding. The most popular method for training embeddings is called word2vec, which is an unsupervised method of training embeddings from large natural language corpora.\n",
        "\n",
        "\n",
        "`word2vec` literature:\n",
        "  - Mikolov, T.,  Chen, K., Corrado, G., & Dean, J. (2013). [Efficient estimation of word representations in vector space](https://arxiv.org/abs/1301.3781). Corr abs/1301.3781.\n",
        "  - Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). [Distributed representations of words and phrases and their compositionally](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). *Advances in neural information processing systems*. 2013.\n",
        "\n",
        "\n",
        "- Other variants of embeddings training:\n",
        "  - `fasttext` from Facebook\n",
        "  - `GloVe` from Stanford NLP Group\n",
        "- There are many ways to train work embeddings.\n",
        "  - `gensim`: Simplest and straightforward implementation of `word2vec`.\n",
        "  - Training based on deep learning packages (e.g., `keras`, `tensorflow`)\n",
        "  - `spacy` (It comes with the pre-trained embeddings models, using GloVe.)\n",
        "- See Sarkar (2019), Chapter 4, for more comprehensive reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uPkYI6PlNgp"
      },
      "source": [
        "If we assume our training corpus contains 10,000 unique words, our input vector to the model to train embeddings with skipgram has 10,000 dimensions, one for each word in the vocabulary.\n",
        "\n",
        "In the input vector all dimensions are zero but one, which indicates the word in the vocabulary, e.g. \"ant\" in the example below. This is why these vectors are called one-hot encodings.\n",
        "\n",
        "\n",
        "Source of the following image: [Chris McCormik Tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDlljZFIliwD"
      },
      "source": [
        "![architecture](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfcYOrYDmpfv"
      },
      "source": [
        "When multiplying a matrix on the hidden layer with the one-hot encoding, we obtain one row of the matrix. So the matrix serves as a lookup table for embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz090_Qen13m"
      },
      "source": [
        "![matrix_mult](http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysBasqzNn5yP"
      },
      "source": [
        "The output layer is a softmax regression classifier, which changes the rates of the weights into a range between 0 and 1, where the sum of all dimensions add up to 1. The final output layer indicates the probabilty of context words for the input, e.g. how high is the probability of \"ability\" occuring near to \"ant\".\n",
        "\n",
        "Since learning embeddings based on all context words anywhere near a word in a text is inefficient, a window size is selected to limit the number of context words that are considered during training:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwnSyAtChC4z"
      },
      "source": [
        "![word2vec_skipgrams](https://tensorflow.org/text/tutorials/images/word2vec_skipgram.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRS9y_W1pv_C"
      },
      "source": [
        "### Using Pre-Trained Embeddings\n",
        "\n",
        "The code below exemplifies how to load a trained embedding model in the gensim library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-K0JS2yIf9ey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82867187-dd28-48f9-aa9a-044564afc521"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-03 11:16:56--  https://github.com/dgromann/SemanticComputing/raw/master/tutorial6/word2vec_embeddings.bin\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dgromann/SemanticComputing/master/tutorial6/word2vec_embeddings.bin [following]\n",
            "--2023-12-03 11:16:57--  https://raw.githubusercontent.com/dgromann/SemanticComputing/master/tutorial6/word2vec_embeddings.bin\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 96769269 (92M) [application/octet-stream]\n",
            "Saving to: â€˜word2vec_embeddings.bin.1â€™\n",
            "\n",
            "word2vec_embeddings 100%[===================>]  92.29M   239MB/s    in 0.4s    \n",
            "\n",
            "2023-12-03 11:16:57 (239 MB/s) - â€˜word2vec_embeddings.bin.1â€™ saved [96769269/96769269]\n",
            "\n",
            "--2023-12-03 11:16:57--  https://raw.githubusercontent.com/dgromann/cl_intro/master/tutorials/Tutorial3.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 272272 (266K) [text/plain]\n",
            "Saving to: â€˜Tutorial3.txt.1â€™\n",
            "\n",
            "Tutorial3.txt.1     100%[===================>] 265.89K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-12-03 11:16:57 (9.94 MB/s) - â€˜Tutorial3.txt.1â€™ saved [272272/272272]\n",
            "\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ],
      "source": [
        "# Let's first load a small subset of word2vec embeddings that have been trained on a\n",
        "# large corpus of news documents\n",
        "!wget https://github.com/dgromann/SemanticComputing/raw/master/tutorial6/word2vec_embeddings.bin\n",
        "!wget https://raw.githubusercontent.com/dgromann/cl_intro/master/tutorials/Tutorial3.txt\n",
        "!pip3 install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uT3equb6aKNy"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Let's load the model\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(\"word2vec_embeddings.bin.1\", binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the length fo the whole vocabulary\n",
        "print(\"Length of the vocabulary\",len(model.key_to_index))\n",
        "\n",
        "# Print the embedding of a specific word\n",
        "print(\"Embedding for the word good: \", model[\"good\"])"
      ],
      "metadata": {
        "id": "D_Rm7ukcl0Yc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "758f328f-219e-4275-d14f-23d233340edd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the vocabulary 80000\n",
            "Embedding for the word good:  [ 0.04052734  0.0625     -0.01745605  0.07861328  0.03271484 -0.01263428\n",
            "  0.00964355  0.12353516 -0.02148438  0.15234375 -0.05834961 -0.10644531\n",
            "  0.02124023  0.13574219 -0.13183594  0.17675781  0.27148438  0.13769531\n",
            " -0.17382812 -0.14160156 -0.03076172  0.19628906 -0.03295898  0.125\n",
            "  0.25390625  0.12695312 -0.15234375  0.03198242  0.01135254 -0.01361084\n",
            " -0.12890625  0.01019287  0.23925781 -0.08447266  0.140625    0.13085938\n",
            " -0.04516602  0.06494141  0.02539062  0.05615234  0.24609375 -0.20507812\n",
            "  0.23632812 -0.00860596 -0.02294922  0.05078125  0.10644531 -0.03564453\n",
            "  0.08740234 -0.05712891  0.08496094  0.23535156 -0.10107422 -0.03564453\n",
            " -0.04736328  0.04736328 -0.14550781 -0.10986328  0.14746094 -0.23242188\n",
            " -0.07275391  0.19628906 -0.37890625 -0.07226562  0.04833984  0.11914062\n",
            "  0.06103516 -0.12109375 -0.27929688  0.05200195  0.04907227 -0.02709961\n",
            "  0.1328125   0.03369141 -0.32226562  0.04223633 -0.08789062  0.15429688\n",
            "  0.09472656  0.10351562 -0.02856445  0.00128174 -0.00427246  0.24609375\n",
            " -0.05957031 -0.16894531 -0.09619141  0.16796875  0.0133667   0.04882812\n",
            "  0.08349609  0.06347656 -0.00872803 -0.08642578 -0.03857422 -0.08251953\n",
            "  0.15722656  0.22753906 -0.00762939 -0.19921875 -0.06347656  0.12792969\n",
            " -0.06347656 -0.03027344  0.0456543   0.06298828 -0.02526855 -0.06787109\n",
            " -0.01141357 -0.13574219  0.02978516  0.10400391 -0.15917969 -0.08447266\n",
            "  0.29882812 -0.12597656  0.11425781 -0.08105469 -0.09082031 -0.07910156\n",
            " -0.11181641 -0.09619141  0.02770996  0.14257812 -0.26757812 -0.09375\n",
            "  0.03979492 -0.17871094 -0.02819824  0.01464844 -0.31640625 -0.24511719\n",
            " -0.08935547  0.09716797 -0.00964355 -0.14746094  0.15234375  0.21582031\n",
            "  0.05981445  0.23828125 -0.05151367  0.14941406  0.13574219 -0.03222656\n",
            " -0.265625   -0.11181641 -0.23046875 -0.140625    0.25585938 -0.15429688\n",
            "  0.1796875   0.15527344 -0.21582031  0.36328125 -0.1015625   0.04980469\n",
            "  0.07177734 -0.14550781 -0.03198242  0.00952148 -0.12109375  0.12109375\n",
            "  0.09765625  0.07763672  0.3203125  -0.22265625 -0.08447266 -0.10742188\n",
            "  0.11279297 -0.13867188 -0.21875     0.0145874   0.13378906 -0.00921631\n",
            "  0.00921631  0.16894531  0.16894531 -0.078125   -0.00665283  0.03735352\n",
            " -0.10888672 -0.25390625  0.01452637 -0.09716797 -0.19628906 -0.01782227\n",
            " -0.28125    -0.02050781 -0.02905273 -0.09375    -0.17675781  0.21484375\n",
            " -0.05224609 -0.11572266 -0.01977539 -0.10839844 -0.01342773 -0.15332031\n",
            " -0.140625   -0.11816406  0.09228516  0.109375    0.05761719 -0.03466797\n",
            "  0.03564453 -0.12011719 -0.14257812 -0.00072479 -0.06689453  0.11914062\n",
            " -0.10449219  0.07861328 -0.12792969  0.09570312 -0.00817871  0.07128906\n",
            "  0.20703125 -0.03149414  0.09570312  0.17285156 -0.07958984 -0.02429199\n",
            " -0.07519531 -0.07568359  0.09521484 -0.06494141 -0.00689697 -0.09033203\n",
            "  0.03100586  0.19921875 -0.10644531 -0.11474609  0.18652344 -0.05078125\n",
            "  0.0859375   0.00128937 -0.18847656 -0.20019531 -0.02832031  0.11328125\n",
            "  0.25976562  0.22070312  0.04101562  0.00171661  0.07568359 -0.01196289\n",
            "  0.0177002  -0.05883789 -0.25976562 -0.234375   -0.04956055  0.25976562\n",
            "  0.15332031  0.15136719  0.08300781 -0.15527344  0.04931641  0.07519531\n",
            " -0.05078125 -0.1328125  -0.13574219  0.04199219 -0.14257812  0.02099609\n",
            "  0.07861328  0.01611328  0.01623535 -0.21582031  0.01599121 -0.04882812\n",
            " -0.02404785  0.13476562  0.08496094 -0.01196289  0.10009766 -0.13867188\n",
            "  0.08056641 -0.22070312 -0.12011719  0.18945312  0.05444336 -0.05053711\n",
            "  0.00147247  0.14160156 -0.06494141 -0.05566406 -0.09033203 -0.0267334\n",
            " -0.10498047  0.02416992  0.01422119  0.1875     -0.16503906  0.01538086\n",
            " -0.04174805  0.05444336 -0.01184082 -0.15625     0.00193024 -0.06982422]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ How many dimensions (numbers) does each vector in this trained embedding model have? Try to find this out with code, not by counting."
      ],
      "metadata": {
        "id": "8Nb8E4AYl6SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code goes here\n",
        "embedding_for_good = model[\"good\"]\n",
        "\n",
        "dimensions = embedding_for_good.shape     # Checks the shape of the embedding\n",
        "print(\"Number of dimensions:\", dimensions)    # each vector in the word2vec embedding model has 300 dimensions\n",
        "\n",
        "# the comma is there to denote that it's a tuple with a single element."
      ],
      "metadata": {
        "id": "zJdLUqzamYIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12257b6f-a95b-4e35-fa4f-8cd5153101e7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of dimensions: (300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use embeddings to evaluate how similar two words are.\n",
        "\n",
        "ðŸ‘‹ âš’ How can we get the first most similiar word of good from the list of the top 5 most similar words?"
      ],
      "metadata": {
        "id": "z0wPsvgHl3wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.most_similar('good'))\n",
        "\n",
        "# Get the top 5 most similar words of \"good\"\n",
        "most_similar = model.most_similar(\"good\", topn=5)\n",
        "print(most_similar)\n",
        "\n",
        "# Your code to get the first word from that list of top 5\n",
        "first_most_similar_word = most_similar[0][0]\n",
        "print(\"First most similar word to 'good':\", first_most_similar_word)\n"
      ],
      "metadata": {
        "id": "hMaZSp_Wl34h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f4e9a4b-6d52-41cd-f050-ca50703f152d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('great', 0.7291510105133057), ('bad', 0.7190051078796387), ('terrific', 0.6889115571975708), ('decent', 0.6837348341941833), ('nice', 0.6836092472076416), ('excellent', 0.644292950630188), ('fantastic', 0.6407778263092041), ('better', 0.6120728850364685), ('solid', 0.5806034803390503), ('lousy', 0.576420247554779)]\n",
            "[('great', 0.7291510105133057), ('bad', 0.7190051078796387), ('terrific', 0.6889115571975708), ('decent', 0.6837348341941833), ('nice', 0.6836092472076416)]\n",
            "First most similar word to 'good': great\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use these embeddings to obtain similar words in other pairs with the analogy task a is to b as c is to d, e.g. *man is to woman as king is to ?*"
      ],
      "metadata": {
        "id": "y4jr1MyxnaKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether our embeddings are good at the analogy task\n",
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yfGt0BDl4AT",
        "outputId": "c84a37c2-f537-4b63-b6e2-2f05124cb726"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.7118193507194519)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ How can we get the first most similiar word of good from the list of the top 5 most similar words?"
      ],
      "metadata": {
        "id": "rGaa-MRunjUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analogy(a, b, c, model, topn=1):\n",
        "    # Calculate the analogy: a is to b as c is to d\n",
        "    result = model.most_similar(positive=[c, b], negative=[a], topn=topn)\n",
        "\n",
        "    result_word = result[0][0]\n",
        "\n",
        "    return result_word\n",
        "\n",
        "# Example usage\n",
        "print(analogy(\"France\", \"Paris\", \"Austria\", model))\n",
        "print(analogy(\"good\", \"best\", \"bad\", model, topn=5))"
      ],
      "metadata": {
        "id": "AvTWf91UlAnZ",
        "outputId": "5cff8710-5a28-434c-eae8-12695ff72698",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vienna\n",
            "worst\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analogy(a, b, c, model, topn=1):\n",
        "  #Your code goes here\n",
        "  result = model.most_similar(positive=[c, b], negative=[a], topn=topn)\n",
        "\n",
        "  result_word = result[0][0]\n",
        "\n",
        "  return result_word\n",
        "\n",
        "print(analogy(\"France\", \"Paris\", \"Austria\", model))\n",
        "print(analogy(\"good\", \"best\", \"bad\", model))"
      ],
      "metadata": {
        "id": "FINqLoLbnlaB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcfce26e-a262-445a-acfe-09779cdab0b6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vienna\n",
            "worst\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use matplotlib to visualize the proximity of words in vector space."
      ],
      "metadata": {
        "id": "cntAtMnkugSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_pca_scatterplot(model, words):\n",
        "\n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)"
      ],
      "metadata": {
        "id": "k5VAQte-pzGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_pca_scatterplot(model,\n",
        "                        ['coffee', 'tea', 'beer', 'wine', 'water',\n",
        "                         'hamburger', 'pizza',  'sushi', 'meatballs',\n",
        "                         'dog', 'horse', 'cat', 'monkey', 'parrot', 'lizard',\n",
        "                         'France', 'Germany', 'Hungary',\n",
        "                         'school', 'college', 'university', 'institute'])"
      ],
      "metadata": {
        "id": "fZGXtTzGqG3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ Write a function that runs through the text file analogy.txt and test your analogy function on each line apart from the header."
      ],
      "metadata": {
        "id": "PaSB9trppFER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analogy = open(\"Tutorial3.txt\", \"r\")\n",
        "\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "55O6Hvr1pJ4T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}