{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NastasiaMazur/VU_1_1/blob/main/tutorials/Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 2: Introduction to Computational Linguistics\n",
        "\n",
        "This is the second tutorial with practical exercises for the lecture Introduction to Computational Linguistics in the winter semester 2023. Hands-on exercises are marked with üëã ‚öí and questions are marked with ‚ùì. Remember to first **store this notebook** in your Drive or GitHub.\n",
        "\n",
        "Today's focus is on the traditional NLP processing pipeline, for which we will be using [spaCy](https://spacy.io/) and [Natural Language Toolkit (NLTK)](https://www.nltk.org/)."
      ],
      "metadata": {
        "id": "h38GAB1CbwAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## **Lesson 2: NLP Pipeline**\n",
        "\n",
        "For the NLP pipeline, we will be using three different libraries today: NLTK, [Stanza](http://stanza.run/), and [spaCy](https://spacy.io/). Thus, we first need to install Stanza."
      ],
      "metadata": {
        "id": "vsFEEJJVaC6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza"
      ],
      "metadata": {
        "id": "q8QJyaSNoLh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f29c943c-2d96-45aa-fbff-aceed066b5f7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.6.1-py3-none-any.whl (881 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m881.2/881.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji (from stanza)\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.31.0)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
            "Installing collected packages: emoji, stanza\n",
            "Successfully installed emoji-2.8.0 stanza-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK and spaCy are already available in a standard Colab Notebook, however, we need to download some packages that we will need in NLTK."
      ],
      "metadata": {
        "id": "rVPDtd67oqAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "id": "rzK0cBfWo3H4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aab4234-32cc-44be-b7c8-2178a07e7a7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization and POS Tagging"
      ],
      "metadata": {
        "id": "eLp4jtydnsv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will use NLTK to tokenize and POS tag a sample sentence. The tagset that the Perceptron Tagger uses is the [Penn Treebank tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
        "\n",
        "‚ùì Are the POS tags for the two different uses of *tears* correct? How does their pronunciation differ?\n"
      ],
      "metadata": {
        "id": "zn8fzwacpvv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Part-of-Speech tagger\n",
        "from nltk.tag.perceptron import PerceptronTagger\n",
        "\n",
        "tagger = PerceptronTagger()\n",
        "\n",
        "# Example sentences\n",
        "sentence = \"It just tears me apart to see you suffering like that and in tears.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "print(word_tokenize(sentence))\n",
        "# POS tag each token in the tokenized sentence\n",
        "pos_tags = tagger.tag(word_tokenize(sentence))\n",
        "print(\"Part of speech tags of the sentence: \", pos_tags)"
      ],
      "metadata": {
        "id": "7CodWjelnhdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c180b9-08ae-46d7-c301-8b7194986fd1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It', 'just', 'tears', 'me', 'apart', 'to', 'see', 'you', 'suffering', 'like', 'that', 'and', 'in', 'tears', '.']\n",
            "Part of speech tags of the sentence:  [('It', 'PRP'), ('just', 'RB'), ('tears', 'VBZ'), ('me', 'PRP'), ('apart', 'RB'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('suffering', 'VBG'), ('like', 'IN'), ('that', 'DT'), ('and', 'CC'), ('in', 'IN'), ('tears', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëã ‚öí Let's do the same in spaCy. Go to the [spaCy documentation](https://spacy.io/usage/linguistic-features) and perform tokenization and POS tagging on the same example sentence. Attention: Only output the tokens, their spaCy internal POS label and the Penn Treebank tags."
      ],
      "metadata": {
        "id": "Fe5ZT8-Oq0Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Your code here\n",
        "doc = nlp(sentence)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.tag_)\n",
        "\n",
        "#for token in doc:\n",
        "#    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "#            token.shape_, token.is_alpha, token.is_stop)\n"
      ],
      "metadata": {
        "id": "436PBflCrDl-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a539d8-b740-45b9-9710-3e0613b8da7a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It PRON PRP\n",
            "just ADV RB\n",
            "tears VERB VBZ\n",
            "me PRON PRP\n",
            "apart ADV RB\n",
            "to PART TO\n",
            "see VERB VB\n",
            "you PRON PRP\n",
            "suffering VERB VBG\n",
            "like ADP IN\n",
            "that PRON DT\n",
            "and CCONJ CC\n",
            "in ADP IN\n",
            "tears NOUN NNS\n",
            ". PUNCT .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization and Stemming\n",
        "\n",
        "We have looked at the comparison between these two in the lecture. Now it is time for you to play around with the two yourself.\n",
        "\n",
        "üëã ‚öí Which stemmer worked better? Which method would you prefer to determine word frequency information of a text corpus?"
      ],
      "metadata": {
        "id": "LlcEyD7_sed0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Stemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Selection of stemmers\n",
        "ps = PorterStemmer()\n",
        "ls = LancasterStemmer()\n",
        "ss = SnowballStemmer(\"english\")\n",
        "\n",
        "# Exercise: Lemmatize and stem (maybe try different stemmers) the following words\n",
        "words = ['presumably', 'provisions', 'owed', 'abacus', 'flies', 'dies', 'mules',\n",
        "        'seizing', 'caresses', 'sensational', 'colonizer', 'traditional', 'plotted']\n",
        "\n",
        "for word in words:\n",
        "  print(lemmatizer.lemmatize(word))\n",
        "  print(ps.stem(word))\n",
        "  print(ls.stem(word))\n",
        "  print(ss.stem(word))\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "icGJ-AhCshbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f36b2f3a-1497-4025-b4e7-65f4da7042b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "presumably\n",
            "presum\n",
            "presum\n",
            "presum\n",
            "provision\n",
            "provis\n",
            "provid\n",
            "provis\n",
            "owed\n",
            "owe\n",
            "ow\n",
            "owe\n",
            "abacus\n",
            "abacu\n",
            "abac\n",
            "abacus\n",
            "fly\n",
            "fli\n",
            "fli\n",
            "fli\n",
            "dy\n",
            "die\n",
            "die\n",
            "die\n",
            "mule\n",
            "mule\n",
            "mul\n",
            "mule\n",
            "seizing\n",
            "seiz\n",
            "seiz\n",
            "seiz\n",
            "caress\n",
            "caress\n",
            "caress\n",
            "caress\n",
            "sensational\n",
            "sensat\n",
            "sens\n",
            "sensat\n",
            "colonizer\n",
            "colon\n",
            "colon\n",
            "colon\n",
            "traditional\n",
            "tradit\n",
            "tradit\n",
            "tradit\n",
            "plotted\n",
            "plot\n",
            "plot\n",
            "plot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With spaCy the code is very much the same for lemmatization as for tokenization and POS tagging, exemplified for our example sentence below. The library, unfortunately, has no function for stemming."
      ],
      "metadata": {
        "id": "EZqSI-iIs1E_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(sentence)\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_)"
      ],
      "metadata": {
        "id": "AuDRsRpItCZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named Entity Recognition (NER)\n",
        "\n",
        "üëã ‚öí Get the results for NER for the following example sentence in spaCy."
      ],
      "metadata": {
        "id": "Wx1hp_Fvux4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_sentence = \"Vienna is lovely in December.\"\n",
        "\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "p5Fy3XIgvEu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependency Parsing\n",
        "\n",
        "Whenever grammatical relations are needed, dependency parsing is very useful. The most common tagset are the [Universal Dependency Relations](https://universaldependencies.org/u/dep/).\n",
        "\n",
        "While there are some options for dependency parsing in NLTK, the successful ones depend on the Stanford Parser. However, Stanza is the more recent version of the Stanford Parser and therefore more useful."
      ],
      "metadata": {
        "id": "VEqPafcmtm4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(example_sentence)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
        "            [child for child in token.children])"
      ],
      "metadata": {
        "id": "GuQ2q9Gct7QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also visualize the dependency relations\n",
        "from spacy import displacy\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "id": "ip0RdHDUxPOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Stanza, we can do all of the above operations in one pipeline. Also spaCy offers a pipeline solution and the combination of several of these parsers in one go."
      ],
      "metadata": {
        "id": "FfizFXSpwNvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "stanza.download('en')"
      ],
      "metadata": {
        "id": "ND90y3fHymeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üëã ‚öí The print statement contains two for loops and an if/else statement. Try to split it up from a one-line code back to the two loops and the statement in several lines."
      ],
      "metadata": {
        "id": "K4N7iVza1iaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = stanza.Pipeline(lang='en', processor='tokenize,pos,lemma,depparse')\n",
        "doc = pipeline(example_sentence)\n",
        "\n",
        "# Try to split the following line into two for statements and one if/else\n",
        "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
      ],
      "metadata": {
        "id": "sprdO5z4wQ3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùì Do you notice any differences between the two types of dependency relations and the output for this sentence? Do the two parsers agree on the existing relations in this sentence?\n"
      ],
      "metadata": {
        "id": "G0sYkUHW1wXS"
      }
    }
  ]
}